{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discourse Similarity Clustering Analyses for Climate Tweets\n",
    "\n",
    "This notebook constructs, tests, and explains the Discouse Similarity Clustering analyses used in the research project on how climte tweets differ between expert and general population users. This method has the following steps:\n",
    "\n",
    "1. **Prepare Texts**: \n",
    "    \n",
    "    Each user's tweets are summarised together and all numbers and links are dropped. \n",
    "\n",
    "2. **Similarity Scores**: \n",
    "    \n",
    "    Between each pair of users, their summarised texts are compared and a similarity score is calculated. This score can be calculated in many ways, but here we first use \"cosine similarity\", the NLP industry standard for similarity computation. Then we perform a robustness check by first using a sentence transformer to extract phrases and then compute cosine similarity on the lists of phrases of all pairs of users. \n",
    "\n",
    "3. **Graph Clustering**: \n",
    "\n",
    "    With the similarity scores between each user pair, we can create a Similarity Graph where each dot represent a user and each link between two dots represent that the pair of users have similar tweets. \n",
    "    \n",
    "    The strengths of the links may represent the level of similarity, and a cut-off can be added to focus only on the stronger links (a rigorous approach will be to use Bayesian Information Criteria but usually we may heuristically use $ \\frac{N^2}{6} $ number of links in a graph of $N$ nodes). \n",
    "    \n",
    "    When a graph is created, we can use graph partitioning algorithms to see if there are structually separated communities, i.e., if there are clusters of users that are more similar to each other than to others in terms of their tweets. By checking if each community have different characteristics such as opinions, we can see if the population is segregated into \"echo chambers\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/_3t1v75j4kz0dzqsw61wxm1w0000gp/T/ipykernel_2217/3472217054.py:13: DtypeWarning: Columns (1) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  Raw_Tweets = pd.read_csv(f\"{file_path}anonhayhoetw.csv\", usecols=['tweet_id', 'author_id', 'clean_text'])\n"
     ]
    }
   ],
   "source": [
    "# initialisation\n",
    "import pandas as pd\n",
    "\n",
    "# read in file and reformat\n",
    "# if there's an error when run on a windows machine, try changing the encoding parameter\n",
    "Raw_Tweets = pd.read_csv(\"data/anonhayhoetw.csv\", encoding='mac_roman',usecols=['tweet_id','author_id','text2'])\n",
    "Raw_Tweets.columns = ['tweet_id','author_id','text']\n",
    "\n",
    "# run this if text already cleaned:\n",
    "Raw_Tweets = pd.read_csv(\"data/anonhayhoetw.csv\", usecols=['tweet_id', 'author_id', 'clean_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 18:51:30,182\tINFO worker.py:1518 -- Started a local Ray instance.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "    <div style=\"margin-left: 50px;display: flex;flex-direction: row;align-items: center\">\n",
       "        <h3 style=\"color: var(--jp-ui-font-color0)\">Ray</h3>\n",
       "        <svg version=\"1.1\" id=\"ray\" width=\"3em\" viewBox=\"0 0 144.5 144.6\" style=\"margin-left: 3em;margin-right: 3em\">\n",
       "            <g id=\"layer-1\">\n",
       "                <path fill=\"#00a2e9\" class=\"st0\" d=\"M97.3,77.2c-3.8-1.1-6.2,0.9-8.3,5.1c-3.5,6.8-9.9,9.9-17.4,9.6S58,88.1,54.8,81.2c-1.4-3-3-4-6.3-4.1\n",
       "                    c-5.6-0.1-9.9,0.1-13.1,6.4c-3.8,7.6-13.6,10.2-21.8,7.6C5.2,88.4-0.4,80.5,0,71.7c0.1-8.4,5.7-15.8,13.8-18.2\n",
       "                    c8.4-2.6,17.5,0.7,22.3,8c1.3,1.9,1.3,5.2,3.6,5.6c3.9,0.6,8,0.2,12,0.2c1.8,0,1.9-1.6,2.4-2.8c3.5-7.8,9.7-11.8,18-11.9\n",
       "                    c8.2-0.1,14.4,3.9,17.8,11.4c1.3,2.8,2.9,3.6,5.7,3.3c1-0.1,2,0.1,3,0c2.8-0.5,6.4,1.7,8.1-2.7s-2.3-5.5-4.1-7.5\n",
       "                    c-5.1-5.7-10.9-10.8-16.1-16.3C84,38,81.9,37.1,78,38.3C66.7,42,56.2,35.7,53,24.1C50.3,14,57.3,2.8,67.7,0.5\n",
       "                    C78.4-2,89,4.7,91.5,15.3c0.1,0.3,0.1,0.5,0.2,0.8c0.7,3.4,0.7,6.9-0.8,9.8c-1.7,3.2-0.8,5,1.5,7.2c6.7,6.5,13.3,13,19.8,19.7\n",
       "                    c1.8,1.8,3,2.1,5.5,1.2c9.1-3.4,17.9-0.6,23.4,7c4.8,6.9,4.6,16.1-0.4,22.9c-5.4,7.2-14.2,9.9-23.1,6.5c-2.3-0.9-3.5-0.6-5.1,1.1\n",
       "                    c-6.7,6.9-13.6,13.7-20.5,20.4c-1.8,1.8-2.5,3.2-1.4,5.9c3.5,8.7,0.3,18.6-7.7,23.6c-7.9,5-18.2,3.8-24.8-2.9\n",
       "                    c-6.4-6.4-7.4-16.2-2.5-24.3c4.9-7.8,14.5-11,23.1-7.8c3,1.1,4.7,0.5,6.9-1.7C91.7,98.4,98,92.3,104.2,86c1.6-1.6,4.1-2.7,2.6-6.2\n",
       "                    c-1.4-3.3-3.8-2.5-6.2-2.6C99.8,77.2,98.9,77.2,97.3,77.2z M72.1,29.7c5.5,0.1,9.9-4.3,10-9.8c0-0.1,0-0.2,0-0.3\n",
       "                    C81.8,14,77,9.8,71.5,10.2c-5,0.3-9,4.2-9.3,9.2c-0.2,5.5,4,10.1,9.5,10.3C71.8,29.7,72,29.7,72.1,29.7z M72.3,62.3\n",
       "                    c-5.4-0.1-9.9,4.2-10.1,9.7c0,0.2,0,0.3,0,0.5c0.2,5.4,4.5,9.7,9.9,10c5.1,0.1,9.9-4.7,10.1-9.8c0.2-5.5-4-10-9.5-10.3\n",
       "                    C72.6,62.3,72.4,62.3,72.3,62.3z M115,72.5c0.1,5.4,4.5,9.7,9.8,9.9c5.6-0.2,10-4.8,10-10.4c-0.2-5.4-4.6-9.7-10-9.7\n",
       "                    c-5.3-0.1-9.8,4.2-9.9,9.5C115,72.1,115,72.3,115,72.5z M19.5,62.3c-5.4,0.1-9.8,4.4-10,9.8c-0.1,5.1,5.2,10.4,10.2,10.3\n",
       "                    c5.6-0.2,10-4.9,9.8-10.5c-0.1-5.4-4.5-9.7-9.9-9.6C19.6,62.3,19.5,62.3,19.5,62.3z M71.8,134.6c5.9,0.2,10.3-3.9,10.4-9.6\n",
       "                    c0.5-5.5-3.6-10.4-9.1-10.8c-5.5-0.5-10.4,3.6-10.8,9.1c0,0.5,0,0.9,0,1.4c-0.2,5.3,4,9.8,9.3,10\n",
       "                    C71.6,134.6,71.7,134.6,71.8,134.6z\"/>\n",
       "            </g>\n",
       "        </svg>\n",
       "        <table>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Python version:</b></td>\n",
       "                <td style=\"text-align: left\"><b>3.10.7</b></td>\n",
       "            </tr>\n",
       "            <tr>\n",
       "                <td style=\"text-align: left\"><b>Ray version:</b></td>\n",
       "                <td style=\"text-align: left\"><b> 2.0.0</b></td>\n",
       "            </tr>\n",
       "            \n",
       "        </table>\n",
       "    </div>\n",
       "</div>\n"
      ],
      "text/plain": [
       "RayContext(dashboard_url='', python_version='3.10.7', ray_version='2.0.0', ray_commit='cba26cc83f6b5b8a2ff166594a65cb74c0ec8740', address_info={'node_ip_address': '127.0.0.1', 'raylet_ip_address': '127.0.0.1', 'redis_address': None, 'object_store_address': '/tmp/ray/session_2022-12-19_18-51-28_509566_2217/sockets/plasma_store', 'raylet_socket_name': '/tmp/ray/session_2022-12-19_18-51-28_509566_2217/sockets/raylet', 'webui_url': '', 'session_dir': '/tmp/ray/session_2022-12-19_18-51-28_509566_2217', 'metrics_export_port': 61101, 'gcs_address': '127.0.0.1:55934', 'address': '127.0.0.1:55934', 'dashboard_agent_listen_port': 52365, 'node_id': '278ddddc5fb35efa2cba1772944f2c8e1b122fb564cb06df8433bfdf'})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# since there are lots of texts to process, we execute some code through multiprocessing using the ray package\n",
    "import ray\n",
    "# change this to the number of cores your computer has\n",
    "ray.init(num_cpus=8) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Cleaning\n",
    "\n",
    "Skip to the next chapter if already done this!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for cleaning texts\n",
    "def clean_text(text):\n",
    "    words = str(text).replace('\\n',' ').replace('\\r',' ').replace(r\"\\\\\",'').split(' ')\n",
    "    words = [\n",
    "        word for word in words\n",
    "        if not any([ch.isdigit() for ch in word])\n",
    "        and not '@' in word\n",
    "        # we should keep the # in because they are meaningful tokens\n",
    "        and not 'http' in word\n",
    "    ]\n",
    "    return ' '.join(words)\n",
    "\n",
    "# one example\n",
    "clean_text(Raw_Tweets.text.values[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a remote wrapper function for bundled execution\n",
    "@ray.remote\n",
    "def ray_clean_texts(bdl_range, bdl_texts):\n",
    "    output = []\n",
    "    start, end = bdl_range\n",
    "    for i in range(start,end):\n",
    "        text = bdl_texts[i-start]\n",
    "        text = clean_text(str(text))\n",
    "        output.append([i, text])\n",
    "    return output\n",
    "\n",
    "# ray bundles\n",
    "# change the size so that you have num_cpus * int number of bundles\n",
    "# e.g.: I initiated 8 CPUs, therefore I want 8, 16, 24, etc bundles\n",
    "BDL_SIZE = 48_000\n",
    "bundles = [(x*BDL_SIZE, (x+1)*BDL_SIZE) for x in range(len(Raw_Tweets)//BDL_SIZE)]\n",
    "bundles.append((bundles[-1][1], len(Raw_Tweets)))\n",
    "print(f\"We have {len(bundles)} bundles for ray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# execute on ray\n",
    "all_texts = Raw_Tweets.text.values\n",
    "\n",
    "print('Sending bundled tasks to ray')\n",
    "ray_handles = []\n",
    "for bdl_range in bundles:\n",
    "    start, end = bdl_range\n",
    "    bdl_texts = all_texts[start:end]\n",
    "    ray_handles.append(ray_clean_texts.remote(bdl_range, bdl_texts))\n",
    "\n",
    "done_handles = []\n",
    "while len(ray_handles):\n",
    "    done, ray_handles = ray.wait(ray_handles)\n",
    "    done_handles.append(done)\n",
    "    print(len(ray_handles)+1, end = \" \")\n",
    "\n",
    "print('')\n",
    "print('Getting from ray to df')\n",
    "\n",
    "del all_texts\n",
    "results = pd.DataFrame(columns=[0,1])\n",
    "for batch in range(int(len(done_handles)/8)):\n",
    "    got_bundles = []\n",
    "    for i in range(batch*8,(batch+1)*8):\n",
    "        got_bundles.append(ray.get(done_handles[i]))\n",
    "        print(len(done_handles)-i, end=' ')\n",
    "    got_bundles = [row for bdl in got_bundles for row in bdl[0]]\n",
    "    results = pd.concat([results, pd.DataFrame(got_bundles)])\n",
    "\n",
    "print('')\n",
    "print('Sorting result df and re-merge')\n",
    "# this step is because ray do things in a suffled order so the results must be re-aligned\n",
    "results = results.sort_values(0)\n",
    "Raw_Tweets['clean_text'] = results[1].values\n",
    "\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# join up the tweets of each user\n",
    "User_Tweets = pd.DataFrame(columns=['author_id', 'text'])\n",
    "User_Tweets.author_id = list(set(Raw_Tweets.author_id.values.astype(int)))\n",
    "User_Tweets = User_Tweets[~User_Tweets.author_id.astype(str).isin(['nan','0'])]\n",
    "User_Tweets.set_index('author_id', inplace=True)\n",
    "\n",
    "for user in User_Tweets.index.values:\n",
    "    User_Tweets.loc[user,'text'] = ' '.join([\n",
    "        str(text) for text in\n",
    "        Raw_Tweets.clean_text.values[Raw_Tweets.author_id == user]\n",
    "    ])\n",
    "    if user % 100 == 0: print(user, end = ' ')\n",
    "\n",
    "User_Tweets.to_csv(\"results/user_tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 1: Cosine Similarity\n",
    "\n",
    "This method is where a cosine similarity is computed directly between each user's tweet summaries, and a graph be constructed on this similarity matrix. Clustering analysis on the graph will then be done in R since it is easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file if starting midway after cleaning is already done\n",
    "User_Tweets = pd.read_csv(\"results/user_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cosine similarity between each user\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create story-word matrix\n",
    "all_tweets = User_Tweets.text.values\n",
    "tweet_word_matrix = TfidfVectorizer(stop_words='english').fit_transform(all_tweets)\n",
    "# should be len(tweet) x len(tokens)\n",
    "print(tweet_word_matrix.shape)\n",
    "\n",
    "# get cosine similarity\n",
    "sim_matrix = cosine_similarity(tweet_word_matrix, tweet_word_matrix)\n",
    "# should be the same shape\n",
    "print(sim_matrix.shape)\n",
    "\n",
    "pd.DataFrame(sim_matrix).to_csv('private/cosine_sim_matrix.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method 2: Key Phrase Similarity\n",
    "\n",
    "In this alternative method, key phrases are first extracted from each user's tweet summaries using a pre-trained sentence transformer. A user-level pairwise cosine similarity is then computed on the phrases as \"the ratio between the intersection and union of the two lists of items\". A graph is then constructed on these similarity scores and clustering analyses be performed in R. \n",
    "\n",
    "Compare to Method 1, this method adds a layer of \"phrases\" instead of using the entire corpus. It takes much longer to run, taking time in terms of hours instead of seconds on a sample of 1000 users with my MacBook Pro M1. In my personal experience, these two methods should produce similar results, so I am including it here as a robustness check. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read in file if starting midway after cleaning is already done\n",
    "User_Tweets = pd.read_csv(\"results/user_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a sentence transformer model for keyword extractions\n",
    "from keybert import KeyBERT\n",
    "keyword_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
    "\n",
    "# function for top keywords\n",
    "def user_keywords(their_tweets, cutoff):\n",
    "    their_keywords = keyword_model.extract_keywords(their_tweets, keyphrase_ngram_range = (1, 3), top_n = 20)\n",
    "    their_keywords = pd.DataFrame(their_keywords)\n",
    "    their_keywords.columns = ['kw','score']\n",
    "    their_keywords = their_keywords.kw.values[their_keywords.score > cutoff]\n",
    "    return ' '.join(their_keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have 72 bundles for ray\n"
     ]
    }
   ],
   "source": [
    "# ray function for speed\n",
    "all_tweets = User_Tweets.text.values\n",
    "\n",
    "@ray.remote\n",
    "def ray_user_kw(bdl_range, bdl_stories):\n",
    "    output = []\n",
    "    start, end = bdl_range\n",
    "    for i in range(start,end):\n",
    "        tweets = bdl_stories[i-start]\n",
    "        output.append([i, user_keywords(tweets, cutoff = 0.3)])\n",
    "    return output\n",
    "\n",
    "# ray bundles\n",
    "BDL_SIZE = 14\n",
    "bundles = [(x*BDL_SIZE, (x+1)*BDL_SIZE) for x in range(len(all_tweets)//BDL_SIZE)]\n",
    "bundles.append((bundles[-1][1], len(all_tweets)))\n",
    "print(f\"We have {len(bundles)} bundles for ray\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sending bundled tasks to ray\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-12-19 18:51:38,894\tWARNING worker.py:1829 -- Warning: The remote function __main__.ray_user_kw is very large (87 MiB). Check that its definition is not implicitly capturing a large array or other object in scope. Tip: use ray.put() to put large objects in the Ray object store.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 \n",
      "Getting from ray to df\n",
      "72 71 70 69 68 67 66 65 64 63 62 61 60 59 58 57 56 55 54 53 52 51 50 49 48 47 46 45 44 43 42 41 40 39 38 37 36 35 34 33 32 31 30 29 28 27 26 25 24 23 22 21 20 19 18 17 16 15 14 13 12 11 10 9 8 7 6 5 4 3 2 1 \n",
      "Sorting result df and remerging... Done\n"
     ]
    }
   ],
   "source": [
    "# execute on ray\n",
    "print('Sending bundled tasks to ray')\n",
    "bdl_handles = []\n",
    "for i, bdl_range in enumerate(bundles):\n",
    "    start, end = bdl_range\n",
    "    bdl_stories = all_tweets[start:end]\n",
    "    bdl_handles.append(ray_user_kw.remote(bdl_range, bdl_stories))\n",
    "    print(len(bdl_handles), end = \" \")\n",
    "\n",
    "print('')\n",
    "print('Getting from ray to df')\n",
    "done_bundles = []\n",
    "while len(bdl_handles):\n",
    "    done, bdl_handles = ray.wait(bdl_handles)\n",
    "    done_bundles.append(ray.get(done))\n",
    "    print(len(bdl_handles)+1, end = \" \")\n",
    "\n",
    "print('')\n",
    "print('Sorting result df and remerging...', end = ' ')\n",
    "all_keywords = [row for bdl in done_bundles for row in bdl[0]]\n",
    "all_keywords = pd.DataFrame(all_keywords)\n",
    "all_keywords.columns = ['i', 'keyword']\n",
    "all_keywords = all_keywords.sort_values('i').reset_index(drop=True)\n",
    "User_Tweets['keywords'] = all_keywords.keyword.values\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "User_Tweets.to_csv(\"results/user_tweets.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 7348)\n",
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "# cosine similarity between each user\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# create story-word matrix\n",
    "all_tweets = User_Tweets.keywords.values\n",
    "tweet_word_matrix = TfidfVectorizer(stop_words='english').fit_transform(all_tweets)\n",
    "# should be len(tweet) x len(tokens)\n",
    "print(tweet_word_matrix.shape)\n",
    "\n",
    "# get cosine similarity\n",
    "sim_matrix = cosine_similarity(tweet_word_matrix, tweet_word_matrix)\n",
    "# should be the same shape\n",
    "print(sim_matrix.shape)\n",
    "\n",
    "pd.DataFrame(sim_matrix).to_csv('private/phrasic_sim_matrix.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
